{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sidhaarthmurali/.local/share/virtualenvs/Task-1-ChatBOT-for-PDFs-w0uFnvha/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from huggingface_hub import hf_hub_download\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.vectorstores import Pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import gradio as gr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOADING THE DOCUMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='What Does BERT Look At?\\nAn Analysis of BERT’s Attention\\nKevin Clark†Urvashi Khandelwal†Omer Levy‡Christopher D. Manning†\\n†Computer Science Department, Stanford University\\n‡Facebook AI Research\\n{kevclark,urvashik,manning }@cs.stanford.edu\\nomerlevy@fb.com\\nAbstract\\nLarge pre-trained neural networks such as\\nBERT have had great recent success in NLP,\\nmotivating a growing body of research investi-\\ngating what aspects of language they are able\\nto learn from unlabeled data. Most recent anal-\\nysis has focused on model outputs (e.g., lan-\\nguage model surprisal) or internal vector rep-\\nresentations (e.g., probing classiﬁers). Com-\\nplementary to these works, we propose meth-\\nods for analyzing the attention mechanisms of\\npre-trained models and apply them to BERT.\\nBERT’s attention heads exhibit patterns such\\nas attending to delimiter tokens, speciﬁc po-\\nsitional offsets, or broadly attending over the\\nwhole sentence, with heads in the same layer\\noften exhibiting similar behaviors. We further\\nshow that certain attention heads correspond\\nwell to linguistic notions of syntax and coref-\\nerence. For example, we ﬁnd heads that at-\\ntend to the direct objects of verbs, determiners\\nof nouns, objects of prepositions, and corefer-\\nent mentions with remarkably high accuracy.\\nLastly, we propose an attention-based probing\\nclassiﬁer and use it to further demonstrate that\\nsubstantial syntactic information is captured in\\nBERT’s attention.\\n1 Introduction\\nLarge pre-trained language models achieve very\\nhigh accuracy when ﬁne-tuned on supervised tasks\\n(Dai and Le, 2015; Peters et al., 2018; Radford\\net al., 2018), but it is not fully understood why.\\nThe strong results suggest pre-training teaches the\\nmodels about the structure of language, but what\\nspeciﬁc linguistic features do they learn?\\nRecent work has investigated this question by\\nexamining the outputs of language models on\\ncarefully chosen input sentences (Linzen et al.,\\n2016) or examining the internal vector representa-\\ntions of the model through methods such as prob-\\ning classiﬁers (Adi et al., 2017; Belinkov et al.,\\n2017). Complementary to these approaches, westudy1theattention maps of a pre-trained model.\\nAttention (Bahdanau et al., 2015) has been a\\nhighly successful neural network component. It is\\nnaturally interpretable because an attention weight\\nhas a clear meaning: how much a particular word\\nwill be weighted when computing the next repre-\\nsentation for the current word. Our analysis fo-\\ncuses on the 144 attention heads in BERT2(De-\\nvlin et al., 2019), a large pre-trained Transformer\\n(Vaswani et al., 2017) network that has demon-\\nstrated excellent performance on many tasks.\\nWe ﬁrst explore generally how BERT’s atten-\\ntion heads behave. We ﬁnd that there are com-\\nmon patterns in their behavior, such as attending to\\nﬁxed positional offsets or attending broadly over\\nthe whole sentence. A surprisingly large amount\\nof BERT’s attention focuses on the deliminator to-\\nken [SEP], which we argue is used by the model\\nas a sort of no-op. Generally we ﬁnd that attention\\nheads in the same layer tend to behave similarly.\\nWe next probe each attention head for linguistic\\nphenomena. In particular, we treat each head as a\\nsimple no-training-required classiﬁer that, given a\\nword as input, outputs the most-attended-to other\\nword. We then evaluate the ability of the heads\\nto classify various syntactic relations. While no\\nsingle head performs well at many relations, we\\nﬁnd that particular heads correspond remarkably\\nwell to particular relations. For example, we ﬁnd\\nheads that ﬁnd direct objects of verbs, determin-\\ners of nouns, objects of prepositions, and objects\\nof possessive pronouns with >75% accuracy. We\\nperform a similar analysis for coreference resolu-\\ntion, also ﬁnding a BERT head that performs quite\\nwell. These results are intriguing because the be-\\nhavior of the attention heads emerges purely from\\nself-supervised training on unlabeled data, without\\nexplicit supervision for syntax or coreference.', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 0}),\n",
       " Document(page_content='havior of the attention heads emerges purely from\\nself-supervised training on unlabeled data, without\\nexplicit supervision for syntax or coreference.\\n1Code will be released at https://github.com/\\nclarkkev/attention-analysis .\\n2We use the English base-sized model.', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 0}),\n",
       " Document(page_content='Head 1-1  Attends broadly   Head 3-1  Attends to next token   Head 8-7  Attends to [SEP]   Head 11-6  Attends to periods   Figure 1: Examples of heads exhibiting the patterns discussed in Section 3. The darkness of a line indicates the\\nstrength of the attention weight (some attention weights are so low they are invisible).\\nOur ﬁndings show that particular heads special-\\nize to speciﬁc aspects of syntax. To get a more\\noverall measure of the attention heads’ syntac-\\ntic ability, we propose an attention-based probing\\nclassiﬁer that takes attention maps as input. The\\nclassiﬁer achieves 77 UAS at dependency pars-\\ning, showing BERT’s attention captures a substan-\\ntial amount about syntax. Several recent works\\nhave proposed incorporating syntactic information\\nto improve attention (Eriguchi et al., 2016; Chen\\net al., 2018; Strubell et al., 2018). Our work sug-\\ngests that to an extent this kind of syntax-aware\\nattention already exists in BERT, which may be\\none of the reason for its success.\\n2 Background: Transformers and BERT\\nAlthough our analysis methods are applicable\\nto any model that uses an attention mechanism,\\nin this paper we analyze BERT (Devlin et al.,\\n2019), a large Transformer (Vaswani et al., 2017)\\nnetwork. Transformers consist of multiple lay-\\ners where each layer contains multiple attention\\nheads. An attention head takes as input a sequence\\nof vectorsh= [h1,...,h n]corresponding to the\\nntokens of the input sentence. Each vector hi\\nis transformed into query, key, and value vectors\\nqi,ki,vithrough separate linear transformations.\\nThe head computes attention weights αbetween\\nall pairs of words as softmax-normalized dot prod-\\nucts between the query and key vectors. The out-\\nputoof the attention head is a weighted sum of the\\nvalue vectors.\\nαij=exp (qT\\nikj)∑n\\nl=1exp (qT\\nikl)oi=n∑\\nj=1αijvjAttention weights can be viewed as governing how\\n“important” every other token is when producing\\nthe next representation for the current token.\\nBERT is pre-trained on 3.3 billion tokens of En-\\nglish text to perform two tasks. In the “masked\\nlanguage modeling” task, the model predicts the\\nidentities of words that have been masked-out of\\nthe input text. In the “next sentence prediction”\\ntask, the model predicts whether the second half\\nof the input follows the ﬁrst half of the input in the\\ncorpus, or is a random paragraph. Further training\\nthe model on supervised data results in impres-\\nsive performance across a variety of tasks rang-\\ning from sentiment analysis to question answering.\\nAn important detail of BERT is the preprocessing\\nused for the input text. A special token [CLS] is\\nadded to the beginning of the text and another to-\\nken [SEP] is added to the end. If the input consists\\nof multiple separate texts (e.g., a reading compre-\\nhension example consists of a separate question\\nand context), [SEP] tokens are also used to sep-\\narate them. As we show in the next section, these\\nspecial tokens play an important role in BERT’s\\nattention. We use the “base” sized BERT model,\\nwhich has 12 layers containing 12 attention heads\\neach. We use <layer>-<head number>to denote\\na particular attention head.\\n3 Surface-Level Patterns in Attention\\nBefore looking at speciﬁc linguistic phenomena,\\nwe ﬁrst perform an analysis of surface-level pat-\\nterns in how BERT’s attention heads behave. Ex-\\namples of heads exhibiting these patterns are\\nshown in Figure 1.', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 1}),\n",
       " Document(page_content='2 4 6 8 10 12\\nLayer0.00.20.40.60.8Avg. Attention\\n[CLS]\\n[SEP]\\n. or ,\\n2 4 6 8 10 12\\nLayer0.00.20.40.60.81.0Avg. Attention\\n[SEP] -> [SEP]\\nother -> [SEP]Figure 2: Each point corresponds to the average atten-\\ntion a particular BERT attention head puts toward a to-\\nken type. Above: heads often attend to “special” to-\\nkens. Early heads attend to [CLS], middle heads attend\\nto [SEP], and deep heads attend to periods and com-\\nmas. Often more than half of a head’s total attention is\\nto these tokens. Below: heads attend to [SEP] tokens\\neven more when the current token is [SEP] itself.\\nSetup. We extract the attention maps from BERT-\\nbase over 1000 random Wikipedia segments. We\\nfollow the setup used for pre-training BERT where\\neach segment consists of at most 128 tokens\\ncorresponding to two consecutive paragraphs of\\nWikipedia (although we do not mask out in-\\nput words or as in BERT’s training). The in-\\nput presented to the model is [CLS] <paragraph-\\n1>[SEP]<paragraph-2>[SEP].\\n3.1 Relative Position\\nFirst, we compute how often BERT’s attention\\nheads attend to the current token, the previous to-\\nken, or the next token. We ﬁnd that most heads\\nput little attention on the current token. However,\\nthere are heads that specialize to attending heavily\\non the next or previous token, especially in ear-\\nlier layers of the network. In particular four atten-\\ntion heads (in layers 2, 4, 7, and 8) on average put\\n>50% of their attention on the previous token and\\nﬁve attention heads (in layers 1, 2, 2, 3, and 6) put\\n>50% of their attention on the next token.\\n2 4 6 8 10 12\\nLayer0.00.51.01.52.02.53.0Average⏐⏐∂L\\n∂α⏐⏐All unmasked tokens\\n[SEP]\\n. or ,Figure 3: Gradient-based feature importance estimates\\nfor attention to [SEP], periods/commas, and other to-\\nkens.\\n2 4 6 8 10 12\\nLayer024Avg. Attention Entropy (nats)\\nuniform attention\\nBERT Heads\\nFigure 4: Entropies of attention distributions. In the\\nﬁrst layer there are particularly high-entropy heads that\\nproduce bag-of-vector-like representations.\\n3.2 Attending to Separator Tokens\\nInterestingly, we found that a substantial amount\\nof BERT’s attention focuses on a few tokens (see\\nFigure 2). For example, over half of BERT’s atten-\\ntion in layers 6-10 focuses on [SEP]. To put this in\\ncontext, since most of our segments are 128 tokens\\nlong, the average attention for a token occurring\\ntwice in a segments like [SEP] would normally be\\naround 1/64. [SEP] and [CLS] are guaranteed to\\nbe present and are never masked out, while pe-\\nriods and commas are the most common tokens\\nin the data excluding “the,” which might be why\\nthe model treats these tokens differently. A sim-\\nilar pattern occurs for the uncased BERT model,\\nsuggesting there is a systematic reason for the at-\\ntention to special tokens rather than it being an ar-\\ntifact of stochastic training.\\nOne possible explanation is that [SEP] is used\\nto aggregate segment-level information which can\\nthen be read by other heads. However, further\\nanalysis makes us doubtful this is the case. If this', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 2}),\n",
       " Document(page_content='explanation were true, we would expect attention\\nheads processing [SEP] to attend broadly over the\\nwhole segment to build up these representations.\\nHowever, they instead almost entirely (more than\\n90%; see bottom of Figure 2) attend to themselves\\nand the other [SEP] token. Furthermore, qualita-\\ntive analysis (see Figure 5) shows that heads with\\nspeciﬁc functions attend to [SEP] when the func-\\ntion is not called for. For example, in head 8-10\\ndirect objects attend to their verbs. For this head,\\nnon-nouns mostly attend to [SEP]. Therefore, we\\nspeculate that attention over these special tokens\\nmight be used as a sort of “no-op” when the atten-\\ntion head’s function is not applicable.\\nTo further investigate this hypothesis, we ap-\\nply gradient-based measures of feature importance\\n(Sundararajan et al., 2017). In particular, we com-\\npute the magnitude of the gradient of the loss from\\nBERT’s masked language modeling task with re-\\nspect to each attention weight. Intuitively, this\\nvalue measures how much changing the attention\\nto a token will change BERT’s outputs. Results\\nare shown in Figure 3. Starting in layer 5 – the\\nsame layer where attention to [SEP] becomes high\\n– the gradients for attention to [SEP] become very\\nsmall. This indicates that attending more or less to\\n[SEP] does not substantially change BERT’s out-\\nputs, supporting the theory that attention to [SEP]\\nis used as a no-op for attention heads.\\n3.3 Focused vs Broad Attention\\nLastly, we measure whether attention heads fo-\\ncus on a few words or attend broadly over many\\nwords. To do this, we compute the average en-\\ntropy of each head’s attention distribution (see\\nFigure 4). We ﬁnd that some attention heads, es-\\npecially in lower layers, have very broad atten-\\ntion. These high-entropy attention heads typically\\nspend at most 10% of their attention mass on any\\nsingle word. The output of these heads is roughly\\na bag-of-vectors representation of the sentence.\\nWe also measured entropies for all attention\\nheads from only the [CLS] token. While the av-\\nerage entropies from [CLS] for most layers are\\nvery close to the ones shown in Figure 4, the\\nlast layer has a high entropy from [CLS] of 3.89\\nnats, indicating very broad attention. This ﬁnd-\\ning makes sense given that the representation for\\nthe [CLS] token is used as input for the “next sen-\\ntence prediction” task during pre-training, so it at-\\ntends broadly to aggregate a representation for thewhole input in the last layer.\\n4 Probing Individual Attention Heads\\nNext, we investigate individual attention heads to\\nprobe what aspects of language they have learned.\\nIn particular, we evaluate attention heads on la-\\nbeled datasets for tasks like dependency parsing.\\nAn overview of our results is shown in Figure 5.\\n4.1 Method\\nWe wish to evaluate attention heads at word-level\\ntasks, but BERT uses byte-pair tokenization (Sen-\\nnrich et al., 2016), which means some words\\n(∼8% in our data) are split up into multiple to-\\nkens. We therefore convert token-token attention\\nmaps to word-word attention maps. For attention\\ntoa split-up word, we sum up the attention weights\\nover its tokens. For attention from a split-up word,\\nwe take the mean of the attention weights over its\\ntokens. These transformations preserve the prop-\\nerty that the attention from each word sums to\\n1. For a given attention head and word, we take\\nwhichever other word receives the most attention\\nweight as that model’s prediction3\\n4.2 Dependency Syntax\\nSetup. We extract attention maps from BERT on\\nthe Wall Street Journal portion of the Penn Tree-\\nbank (Marcus et al., 1993) annotated with Stanford\\nDependencies. We evaluate both “directions” of\\nprediction for each attention head: the head word\\nattending to the dependent and the dependent at-\\ntending to the head word. Some dependency rela-\\ntions are simpler to predict than others: for exam-\\nple a noun’s determiner is often the immediately\\npreceding word. Therefore as a point of compar-\\nison, we show predictions from a simple ﬁxed-', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 3}),\n",
       " Document(page_content='tions are simpler to predict than others: for exam-\\nple a noun’s determiner is often the immediately\\npreceding word. Therefore as a point of compar-\\nison, we show predictions from a simple ﬁxed-\\noffset baseline. For example, a ﬁxed offset of -2\\nmeans the word two positions to the left of the de-\\npendent is always considered to be the head.\\nResults. Table 1 shows that there is no single at-\\ntention head that does well at syntax “overall”; the\\nbest head gets 34.5 UAS, which is not much better\\nthan the right-branching baseline, which gets 26.3\\nUAS. This ﬁnding is similar to the one reported by\\nRaganato and Tiedemann (2018), who also evalu-\\nate individual attention heads for syntax.\\nHowever, we do ﬁnd that certain attention heads\\nspecialize to speciﬁc dependency relations, some-\\n3We ignore [SEP] and [CLS], although in practice this\\ndoes not signiﬁcantly change the accuracies for most heads.', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 3}),\n",
       " Document(page_content='Head 9-6  - Prepositions attend to their objects  - 76.3% accuracy at the pobj relation Head 8-11  - Noun modifiers (e.g., determiners) attend   to their noun  - 94.3% accuracy at the det relation Head 8-10  - Direct objects attend to their verbs  - 86.8% accuracy at the dobj relation \\nHead 7-6  - Possessive pronouns and apostrophes   attend to the head of the corresponding NP  - 80.5% accuracy at the poss relation Head 4-10  - Passive auxiliary verbs attend to the   verb they modify  - 82.5% accuracy at the auxpass relation \\nHead 5-4  - Coreferent mentions attend to their antecedents  - 65.1% accuracy at linking the head of a    coreferent mention to the head of an antecedent Figure 5: BERT attention heads that correspond to linguistic phenomena. In the example attention maps, the\\ndarkness of a line indicates the strength of the attention weight. All attention to/from red words is colored red;\\nthese colors are there to highlight certain parts of the attention heads’ behaviors. For Head 9-6, we don’t show\\nattention to [SEP] for clarity. Despite not being explicitly trained on these tasks, BERT’s attention heads perform\\nremarkably well, illustrating how syntax-sensitive behavior can emerge from self-supervised training alone.', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 4}),\n",
       " Document(page_content='Relation Head Accuracy Baseline\\nAll 7-6 34.5 26.3 (1)\\nprep 7-4 66.7 61.8 (-1)\\npobj 9-6 76.3 34.6 (-2)\\ndet 8-11 94.3 51.7 (1)\\nnn 4-10 70.4 70.2 (1)\\nnsubj 8-2 58.5 45.5 (1)\\namod 4-10 75.6 68.3 (1)\\ndobj 8-10 86.8 40.0 (-2)\\nadvmod 7-6 48.8 40.2 (1)\\naux 4-10 81.1 71.5 (1)\\nposs 7-6 80.5 47.7 (1)\\nauxpass 4-10 82.5 40.5 (1)\\nccomp 8-1 48.8 12.4 (-2)\\nmark 8-2 50.7 14.5 (2)\\nprt 6-7 99.1 91.4 (-1)\\nTable 1: The best performing attentions heads of\\nBERT on WSJ dependency parsing by dependency\\ntype. Numbers after baseline accuracies show the best\\noffset found (e.g., (1) means the word to the right is\\npredicted as the head). We show the 10 most common\\nrelations as well as 5 other ones attention heads do well\\non. Bold highlights particularly effective heads.\\ntimes achieving high accuracy and substantially\\noutperforming the ﬁxed-offset baseline. We ﬁnd\\nthat for all relations in Table 1 except pobj , the\\ndependent attends to the head word rather than the\\nother way around, likely because each dependent\\nhas exactly one head but heads have multiple de-\\npendents. We also note heads can disagree with\\nstandard annotation conventions while still per-\\nforming syntactic behavior. For example, head 7-\\n6 marks ’sas the dependent for the poss relation,\\nwhile gold-standard labels mark the complement\\nof an ’sas the dependent (the accuracy in Table 1\\ncounts ’sas correct). Such disagreements high-\\nlight how these syntactic behaviors in BERT are\\nlearned as a by-product of self-supervised train-\\ning, not by copying a human design.\\nFigure 5 shows some examples of the attention\\nbehavior. While the similarity between machine-\\nlearned attention weights and human-deﬁned syn-\\ntactic relations are striking, we note these are re-\\nlations for which attention heads do particularly\\nwell on. There are many relations for which BERT\\nonly slightly improves over the simple baseline, so\\nwe would not say individual attention heads cap-\\nture dependency structure as a whole. We think\\nit would be interesting future work to extend ouranalysis to see if the relations well-captured by at-\\ntention are similar or different for other languages.\\n4.3 Coreference Resolution\\nHaving shown BERT attention heads reﬂect cer-\\ntain aspects of syntax, we now explore using at-\\ntention heads for the more challenging semantic\\ntask of coreference resolution. Coreference links\\nare usually longer than syntactic dependencies and\\nstate-of-the-art systems generally perform much\\nworse at coreference compared to parsing.\\nSetup. We evaluate the attention heads on coref-\\nerence resolution using the CoNLL-2012 dataset4\\n(Pradhan et al., 2012). In particular, we compute\\nantecedent selection accuracy: what percent of the\\ntime does the head word of a coreferent mention\\nmost attend to the head of one of that mention’s\\nantecedents. We compare against three baselines\\nfor selecting an antecedent:\\n•Picking the nearest other mention.\\n•Picking the nearest other mention with the\\nsame head word as the current mention.\\n•A simple rule-based system inspired by Lee\\net al. (2011). It proceeds through 4 sieves: (1)\\nfull string match, (2) head word match, (3)\\nnumber/gender/person match, (4) all other\\nmentions. The nearest mention satisfying the\\nearliest sieve is returned.\\nWe also show the performance of a recent neural\\ncoreference system from Wiseman et al. (2015).\\nResults. Results are shown in Table 2. We ﬁnd\\nthat one of BERT’s attention heads achieves de-\\ncent coreference resolution performance, improv-\\ning by over 10 accuracy points on the string-\\nmatching baseline and performing close to the\\nrule-based system. It is particularly good with\\nnominal mentions, perhaps because it is capable\\nof fuzzy matching between synonyms as seen in\\nthe bottom right of Figure 5.\\n5 Probing Attention Head Combinations\\nSince individual attention heads specialize to par-\\nticular aspects of syntax, the model’s overall\\n“knowledge” about syntax is distributed across\\nmultiple attention heads. We now measure this\\noverall ability by proposing a novel family of', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 5}),\n",
       " Document(page_content='ticular aspects of syntax, the model’s overall\\n“knowledge” about syntax is distributed across\\nmultiple attention heads. We now measure this\\noverall ability by proposing a novel family of\\nattention-based probing classiﬁers and applying\\n4We truncate documents to 128 tokens long to keep mem-\\nory usage manageable.', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 5}),\n",
       " Document(page_content='Model All Pronoun Proper Nominal\\nNearest 27 29 29 19\\nHead match 52 47 67 40\\nRule-based 69 70 77 60\\nNeural coref 83* – – –\\nHead 5-4 65 64 73 58\\n*Only roughly comparable because on non-truncated docu-\\nments and with different mention detection.\\nTable 2: Accuracies (%) for systems at selecting a\\ncorrect antecedent given a coreferent mention in the\\nCoNLL-2012 data. One of BERT’s attention heads per-\\nforms fairly well at coreference.\\nthem to dependency parsing. For these classiﬁers\\nwe treat the BERT attention outputs as ﬁxed, i.e.,\\nwe do not back-propagate into BERT and only\\ntrain a small number of parameters.\\nThe probing classiﬁers are basically graph-\\nbased dependency parsers. Given an input word,\\nthe classiﬁer produces a probability distribution\\nover other words in the sentence indicating how\\nlikely each other word is to be the syntactic head\\nof the current one.\\nAttention-Only Probe. Our ﬁrst probe learns a\\nsimple linear combination of attention weights.\\np(i|j)∝exp(n∑\\nk=1wkαk\\nij+ukαk\\nji)\\nwherep(i|j)is the probability of word ibeing\\nwordj’s syntactic head, αk\\nijis the attention weight\\nfrom wordito wordjproduced by head k, andn\\nis the number of attention heads. We include both\\ndirections of attention: candidate head to depen-\\ndent as well as dependent to candidate head. The\\nweight vectors wanduare trained using standard\\nsupervised learning on the train set.\\nAttention-and-Words Probe. Given our ﬁnding\\nthat heads specialize to particular syntactic rela-\\ntions, we believe probing classiﬁers should beneﬁt\\nfrom having information about the input words. In\\nparticular, we build a model that sets the weights\\nof the attention heads based on the GloVe (Pen-\\nnington et al., 2014) embeddings for the input\\nwords. Intuitively, if the dependent and candi-\\ndate head are “the” and “cat,” the probing classi-\\nﬁer should learn to assign most of the weight to\\nthe head 8-11, which achieves excellent perfor-\\nmance at the determiner relation. The attention-and-words probing classiﬁer assigns the probabil-\\nity of wordibeing wordj’s head as\\np(i|j)∝exp(n∑\\nk=1Wk,:(vi⊕vj)αk\\nij+\\nUk,:(vi⊕vj)αk\\nji)\\nWherevdenotes GloVe embeddings and ⊕de-\\nnotes concatenation. The GloVe embeddings are\\nheld ﬁxed in training, so only the two weight ma-\\ntricesWandUare learned. The dot product\\nWk,:(vi⊕vj)produces a word-sensitive weight for\\nthe particular attention head.\\nResults. We evaluate our methods on the Penn\\nTreebank dev set annotated with Stanford depen-\\ndencies. We compare against three baselines:\\n•A right-branching baseline that always pre-\\ndicts the head is to the dependent’s right.\\n•A simple one-hidden-layer network that takes\\nas input the GloVe embeddings for the depen-\\ndent and candidate head as well as distance\\nfeatures between the two words.5\\n•Our attention-and-words probe, but with at-\\ntention maps from a BERT network with pre-\\ntrained word/positional embeddings but ran-\\ndomly initialized other weights. This kind of\\nbaseline is surprisingly strong at other prob-\\ning tasks (Conneau et al., 2018).\\nResults are shown in Table 3. We ﬁnd the Attn +\\nGloVe probing classiﬁer substantially outperforms\\nour baselines and achieves a decent UAS of 77,\\nsuggesting BERT’s attention maps have a fairly\\nthorough representation of English syntax.\\nAs a rough comparison, we also report results\\nfrom the structural probe from Hewitt and Man-\\nning (2019), which builds a probing classiﬁer on\\ntop of BERT’s vector representations rather than\\nattention. The scores are not directly compara-\\nble because the structural probe only uses a sin-\\ngle layer of BERT, produces undirected rather than\\ndirected parse trees, and is trained to produce the\\nsyntactic distance between words rather than di-\\nrectly predicting the tree structure. Nevertheless,\\nthe similarity in score to our Attn + Glove probing\\nclassiﬁer suggests there is not much more syntac-\\ntic information in BERT’s vector representations\\ncompared to its attention maps.\\n5Indicator features for short distances as well as continu-', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 6}),\n",
       " Document(page_content='classiﬁer suggests there is not much more syntac-\\ntic information in BERT’s vector representations\\ncompared to its attention maps.\\n5Indicator features for short distances as well as continu-\\nous distance features, with distance ahead/behind treated sep-\\narately to capture word order', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 6}),\n",
       " Document(page_content='Model UAS\\nStructural probe 80 UUAS*\\nRight-branching 26\\nDistances + GloVe 58\\nRandom Init Attn + GloVe 30\\nAttn 61\\nAttn + GloVe 77\\nTable 3: Results of attention-based probing classiﬁers\\non dependency parsing. A simple model taking BERT\\nattention maps and GloVe embeddings as input per-\\nforms quite well. *Not directly comparable to our num-\\nbers; see text.\\nOverall, our results from probing both individ-\\nual and combinations of attention heads suggest\\nthat BERT learns some aspects syntax purely as a\\nby-product of self-supervised training. Other work\\nhas drawn a similar conclusions from examin-\\ning BERT’s predictions on agreement tasks (Gold-\\nberg, 2019) or internal vector representations (He-\\nwitt and Manning, 2019; Liu et al., 2019). Tra-\\nditionally, syntax-aware models have been devel-\\noped through architecture design (e.g., recursive\\nneural networks) or from direct supervision from\\nhuman-curated treebanks. Our ﬁndings are part of\\na growing body of work indicating that indirect\\nsupervision from rich pre-training tasks like lan-\\nguage modeling can also produce models sensitive\\nto language’s hierarchical structure.\\n6 Clustering Attention Heads\\nAre attention heads in the same layer similar to\\neach other or different? Can attention heads be\\nclearly grouped by behavior? We investigate these\\nquestions by computing the distances between all\\npairs of attention heads. Formally, we measure the\\ndistance between two heads H iand H jas:\\n∑\\ntoken∈dataJS(Hi(token ),Hj(token ))\\nWhereJSis the Jensen-Shannon Divergence be-\\ntween attention distributions. Using these dis-\\ntances, we visualize the attention heads by apply-\\ning multidimensional scaling (Kruskal, 1964) to\\nembed each head in two dimensions such that the\\nEuclidean distance between embeddings reﬂects\\nthe Jensen-Shannon distance between the corre-\\nsponding heads as closely as possible.\\nResults are shown in Figure 6. We ﬁnd that\\nthere are several clear clusters of heads that be-\\nFigure 6: BERT attention heads embedded in two-\\ndimensional space. Distances between points approxi-\\nmately match the average Jensen-Shannon divergences\\nbetween the outputs of the corresponding heads. Heads\\nin the same layer tend to be close together. Attention\\nhead “behavior” was found through the analysis meth-\\nods discussed throughout this paper.\\nhave similarly, often corresponding to behaviors\\nwe have already discussed in this paper. Heads\\nwithin the same layer are often fairly close to each\\nother, meaning that heads within the layer have\\nsimilar attention distributions. This ﬁnding is a bit\\nsurprising given that Tu et al. (2018) show that en-\\ncouraging attention heads to have different behav-\\niors can improve Transformer performance at ma-\\nchine translation. One possibility for the apparent\\nredundancy in BERT’s attention heads is the use', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 7}),\n",
       " Document(page_content='of attention dropout, which causes some attention\\nweights to be zeroed-out during training.\\n7 Related Work\\nThere has been substantial recent work perform-\\ning analysis to better understand what neural net-\\nworks learn, especially from language model pre-\\ntraining. One line of research examines the out-\\nputs of language models on carefully chosen in-\\nput sentences (Linzen et al., 2016; Khandelwal\\net al., 2018; Gulordava et al., 2018; Marvin and\\nLinzen, 2018). For example, the model’s perfor-\\nmance at subject-verb agreement (generating the\\ncorrect number of a verb far away from its sub-\\nject) provides a measure of the model’s syntactic\\nability, although it does not reveal how that ability\\nis captured by the network.\\nAnother line of work investigates the internal\\nvector representations of the model (Adi et al.,\\n2017; Giulianelli et al., 2018; Zhang and Bow-\\nman, 2018), often using probing classiﬁers. Prob-\\ning classiﬁers are simple neural networks that take\\nthe vector representations of a pre-trained model\\nas input and are trained to do a supervised task\\n(e.g., part-of-speech tagging). If a probing clas-\\nsiﬁer achieves high accuracy, it suggests that the\\ninput representations reﬂect the corresponding as-\\npect of language (e.g., low-level syntax). Like\\nour work, some of these studies have also demon-\\nstrated models capturing aspects of syntax (Shi\\net al., 2016; Blevins et al., 2018) or coreference\\n(Tenney et al., 2018, 2019; Liu et al., 2019) with-\\nout explicitly being trained for the tasks.\\nWith regards to analyzing attention, Vig (2019)\\nbuilds a visualization tool for the BERT’s atten-\\ntion and reports observations about the attention\\nbehavior, but does not perform quantitative anal-\\nysis. Burns et al. (2018) analyze the attention\\nof memory networks to understand model perfor-\\nmance on a question answering dataset. There has\\nalso been some initial work in correlating atten-\\ntion with syntax. Raganato and Tiedemann (2018)\\nevaluate the attention heads of a machine trans-\\nlation model on dependency parsing, but only re-\\nport overall UAS scores instead of investigating\\nheads for speciﬁc syntactic relations or using prob-\\ning classiﬁers. Marecek and Rosa (2018) propose\\nheuristic ways of converting attention scores to\\nsyntactic trees, but do not quantitatively evaluate\\ntheir approach. For coreference, V oita et al. (2018)\\nshow that the attention of a context-aware neu-ral machine translation system captures anaphora,\\nsimilar to our ﬁnding for BERT.\\nConcurrently with our work V oita et al. (2019)\\nidentify syntactic, positional, and rare-word-\\nsensitive attention heads in machine translation\\nmodels. They also demonstrate that many atten-\\ntion heads can be pruned away without substan-\\ntially hurting model performance. Interestingly,\\nthe important attention heads that remain after\\npruning tend to be ones with identiﬁed behaviors.\\nMichel et al. (2019) similarly show that many of\\nBERT’s attention heads can be pruned. Although\\nour analysis in this paper only found interpretable\\nbehaviors in a subset of BERT’s attention heads,\\nthese recent works suggest that there might not be\\nmuch to explain for some attention heads because\\nthey have little effect on model perfomance.\\nJain and Wallace (2019) argue that attention of-\\nten does not “explain” model predictions. They\\nshow that attention weights frequently do not cor-\\nrelate with other measures of feature importance.\\nFurthermore, attention weights can often be sub-\\nstantially changed without altering model predic-\\ntions. However, our motivation for looking at at-\\ntention is different: rather than explaining model\\npredictions, we are seeking to understand infor-\\nmation learned by the models. For example, if\\na particular attention head learns a syntactic rela-\\ntion, we consider that an important ﬁnding from\\nan analysis perspective even if that head is not\\nalways used when making predictions for some\\ndownstream task.\\n8 Conclusion\\nWe have proposed a series of analysis methods for', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 8}),\n",
       " Document(page_content='an analysis perspective even if that head is not\\nalways used when making predictions for some\\ndownstream task.\\n8 Conclusion\\nWe have proposed a series of analysis methods for\\nunderstanding the attention mechanisms of mod-\\nels and applied them to BERT. While most recent\\nwork on model analysis for NLP has focused on\\nprobing vector representations or model outputs,\\nwe have shown that a substantial amount of lin-\\nguistic knowledge can be found not only in the\\nhidden states, but also in the attention maps. We\\nthink probing attention maps complements these\\nother model analysis techniques, and should be\\npart of the toolkit used by researchers to under-\\nstand what neural networks learn about language.\\nAcknowledgements\\nWe thank the anonymous reviews for their\\nthoughtful comments and suggestions. Kevin is\\nsupported by a Google PhD Fellowship.', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 8}),\n",
       " Document(page_content='References\\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer\\nLavi, and Yoav Goldberg. 2017. Fine-grained anal-\\nysis of sentence embeddings using auxiliary predic-\\ntion tasks. In ICLR .\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\\ngio. 2015. Neural machine translation by jointly\\nlearning to align and translate. In ICLR .\\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Has-\\nsan Sajjad, and James R. Glass. 2017. What do neu-\\nral machine translation models learn about morphol-\\nogy? In ACL.\\nTerra Blevins, Omer Levy, and Luke S. Zettlemoyer.\\n2018. Deep rnns encode soft hierarchical syntax. In\\nACL.\\nKaylee Burns, Aida Nematzadeh, Alison Gopnik, and\\nThomas L. Grifﬁths. 2018. Exploiting attention to\\nreveal shortcomings in memory models. In Black-\\nboxNLP@EMNLP .\\nKehai Chen, Rui Wang, Masao Utiyama, Eiichiro\\nSumita, and Tiejun Zhao. 2018. Syntax-directed at-\\ntention for neural machine translation. In AAAI .\\nAlexis Conneau, Germ ´an Kruszewski, Guillaume\\nLample, Lo ¨ıc Barrault, and Marco Baroni. 2018.\\nWhat you can cram into a single $&!#* vector:\\nProbing sentence embeddings for linguistic proper-\\nties. In ACL.\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In NIPS .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In NAACL-HLT .\\nAkiko Eriguchi, Kazuma Hashimoto, and Yoshimasa\\nTsuruoka. 2016. Tree-to-sequence attentional neu-\\nral machine translation. In ACL.\\nMario Giulianelli, Jack Harding, Florian Mohnert,\\nDieuwke Hupkes, and Willem H. Zuidema. 2018.\\nUnder the hood: Using diagnostic classiﬁers to in-\\nvestigate and improve how language models track\\nagreement information. In BlackboxNLP@EMNLP .\\nYoav Goldberg. 2019. Assessing BERT’s syntactic\\nabilities. arXiv preprint arXiv:1901.05287 .\\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\\nTal Linzen, and Marco Baroni. 2018. Colorless\\ngreen recurrent networks dream hierarchically. In\\nNAACL-HLT .\\nJohn Hewitt and Christopher D. Manning. 2019. Find-\\ning syntax with structural probes. In NAACL-HLT .\\nSarthak Jain and Byron C. Wallace. 2019. Attention is\\nnot explanation. arXiv preprint arXiv:1902.10186 .Urvashi Khandelwal, He He, Peng Qi, and Daniel Ju-\\nrafsky. 2018. Sharp nearby, fuzzy far away: How\\nneural language models use context. In ACL.\\nJoseph B Kruskal. 1964. Multidimensional scaling by\\noptimizing goodness of ﬁt to a nonmetric hypothe-\\nsis.Psychometrika , 29(1):1–27.\\nHeeyoung Lee, Yves Peirsman, Angel Chang,\\nNathanael Chambers, Mihai Surdeanu, and Dan Ju-\\nrafsky. 2011. Stanford’s multi-pass sieve corefer-\\nence resolution system at the conll-2011 shared task.\\nInCoNLL .\\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\\n2016. Assessing the ability of lstms to learn syntax-\\nsensitive dependencies. TACL .\\nNelson F. Liu, Matt Gardner, Yonatan Belinkov, M. Pe-\\nters, and Noah A. Smith. 2019. Linguistic knowl-\\nedge and transferability of contextual representa-\\ntions. In NAACL-HLT .\\nMitchell P Marcus, Mary Ann Marcinkiewicz, and\\nBeatrice Santorini. 1993. Building a large annotated\\ncorpus of english: The Penn treebank. Computa-\\ntional linguistics , 19(2):313–330.\\nDavid Marecek and Rudolf Rosa. 2018. Extract-\\ning syntactic trees from transformer encoder self-\\nattentions. In BlackboxNLP@EMNLP .\\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\\ntactic evaluation of language models. In EMNLP .\\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\\nAre sixteen heads really better than one? arXiv\\npreprint arXiv:1905.10650 .\\nJeffrey Pennington, Richard Socher, and Christopher\\nManning. 2014. Glove: Global vectors for word\\nrepresentation. In EMNLP .\\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\\nGardner, Christopher Clark, Kenton Lee, and Luke\\nZettlemoyer. 2018. Deep contextualized word rep-\\nresentations. In NAACL-HLT .\\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\\nOlga Uryupina, and Yuchen Zhang. 2012. Conll-\\n2012 shared task: Modeling multilingual unre-', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 9}),\n",
       " Document(page_content='resentations. In NAACL-HLT .\\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\\nOlga Uryupina, and Yuchen Zhang. 2012. Conll-\\n2012 shared task: Modeling multilingual unre-\\nstricted coreference in ontonotes. In Joint Confer-\\nence on EMNLP and CoNLL-Shared Task .\\nAlec Radford, Karthik Narasimhan, Tim Salimans,\\nand Ilya Sutskever. 2018. Improving lan-\\nguage understanding by generative pre-training.\\nhttps://blog.openai.com/language-unsupervised .\\nAlessandro Raganato and J ¨org Tiedemann. 2018.\\nAn analysis of encoder representations in\\ntransformer-based machine translation. In Black-\\nboxNLP@EMNLP .\\nRico Sennrich, Barry Haddow, and Alexandra Birch.\\n2016. Neural machine translation of rare words with\\nsubword units. In ACL.', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 9}),\n",
       " Document(page_content='Xing Shi, Inkit Padhi, and Kevin Knight. 2016. Does\\nstring-based neural mt learn source syntax? In\\nEMNLP .\\nEmma Strubell, Patrick Verga, Daniel Andor,\\nDavid I Weiss, and Andrew McCallum. 2018.\\nLinguistically-informed self-attention for semantic\\nrole labeling. In EMNLP .\\nMukund Sundararajan, Ankur Taly, and Qiqi Yan.\\n2017. Axiomatic attribution for deep networks. In\\nICML .\\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\\nBert rediscovers the classical nlp pipeline. arXiv\\npreprint arXiv:1905.05950 .\\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\\nAdam Poliak, R Thomas McCoy, Najoung Kim,\\nBenjamin Van Durme, Samuel R Bowman, Dipan-\\njan Das, et al. 2018. What do you learn from con-\\ntext? probing for sentence structure in contextual-\\nized word representations. In ICLR .\\nZhaopeng Tu, Baosong Yang, Michael R. Lyu, and\\nTong Zhang. 2018. Multi-head attention with dis-\\nagreement regularization. In EMNLP .\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In NIPS .\\nJesse Vig. 2019. Visualizing attention in transformer-\\nbased language models. arXiv preprint\\narXiv:1904.02679 .\\nElena V oita, Pavel Serdyukov, Rico Sennrich, and Ivan\\nTitov. 2018. Context-aware neural machine transla-\\ntion learns anaphora resolution. In ACL.\\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\\nnrich, and Ivan Titov. 2019. Analyzing multi-\\nhead self-attention: Specialized heads do the heavy\\nlifting, the rest can be pruned. arXiv preprint\\narXiv:1905.09418 .\\nSam Joshua Wiseman, Alexander Matthew Rush, Stu-\\nart Merrill Shieber, and Jason Weston. 2015. Learn-\\ning anaphoricity and antecedent ranking features for\\ncoreference resolution. In ACL.\\nKelly W. Zhang and Samuel R. Bowman. 2018. Lan-\\nguage modeling teaches you more syntax than trans-\\nlation does: Lessons learned through auxiliary task\\nanalysis. In BlackboxNLP@EMNLP .', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 10})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_path = \"/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf\"\n",
    "loader = PyPDFLoader(file_path=pdf_path)\n",
    "document = loader.load_and_split()\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPLIT DOCUMENT INTO CHUNKS FOR LESSER API CALLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 600, chunk_overlap = 30)\n",
    "doc = text_splitter.split_documents(documents = document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='What Does BERT Look At?\\nAn Analysis of BERT’s Attention\\nKevin Clark†Urvashi Khandelwal†Omer Levy‡Christopher D. Manning†\\n†Computer Science Department, Stanford University\\n‡Facebook AI Research\\n{kevclark,urvashik,manning }@cs.stanford.edu\\nomerlevy@fb.com\\nAbstract\\nLarge pre-trained neural networks such as\\nBERT have had great recent success in NLP,\\nmotivating a growing body of research investi-\\ngating what aspects of language they are able\\nto learn from unlabeled data. Most recent anal-\\nysis has focused on model outputs (e.g., lan-\\nguage model surprisal) or internal vector rep-', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 0}),\n",
       " Document(page_content='resentations (e.g., probing classiﬁers). Com-\\nplementary to these works, we propose meth-\\nods for analyzing the attention mechanisms of\\npre-trained models and apply them to BERT.\\nBERT’s attention heads exhibit patterns such\\nas attending to delimiter tokens, speciﬁc po-\\nsitional offsets, or broadly attending over the\\nwhole sentence, with heads in the same layer\\noften exhibiting similar behaviors. We further\\nshow that certain attention heads correspond\\nwell to linguistic notions of syntax and coref-\\nerence. For example, we ﬁnd heads that at-\\ntend to the direct objects of verbs, determiners', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 0}),\n",
       " Document(page_content='of nouns, objects of prepositions, and corefer-\\nent mentions with remarkably high accuracy.\\nLastly, we propose an attention-based probing\\nclassiﬁer and use it to further demonstrate that\\nsubstantial syntactic information is captured in\\nBERT’s attention.\\n1 Introduction\\nLarge pre-trained language models achieve very\\nhigh accuracy when ﬁne-tuned on supervised tasks\\n(Dai and Le, 2015; Peters et al., 2018; Radford\\net al., 2018), but it is not fully understood why.\\nThe strong results suggest pre-training teaches the\\nmodels about the structure of language, but what', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 0}),\n",
       " Document(page_content='speciﬁc linguistic features do they learn?\\nRecent work has investigated this question by\\nexamining the outputs of language models on\\ncarefully chosen input sentences (Linzen et al.,\\n2016) or examining the internal vector representa-\\ntions of the model through methods such as prob-\\ning classiﬁers (Adi et al., 2017; Belinkov et al.,\\n2017). Complementary to these approaches, westudy1theattention maps of a pre-trained model.\\nAttention (Bahdanau et al., 2015) has been a\\nhighly successful neural network component. It is\\nnaturally interpretable because an attention weight', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 0}),\n",
       " Document(page_content='has a clear meaning: how much a particular word\\nwill be weighted when computing the next repre-\\nsentation for the current word. Our analysis fo-\\ncuses on the 144 attention heads in BERT2(De-\\nvlin et al., 2019), a large pre-trained Transformer\\n(Vaswani et al., 2017) network that has demon-\\nstrated excellent performance on many tasks.\\nWe ﬁrst explore generally how BERT’s atten-\\ntion heads behave. We ﬁnd that there are com-\\nmon patterns in their behavior, such as attending to\\nﬁxed positional offsets or attending broadly over\\nthe whole sentence. A surprisingly large amount', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 0}),\n",
       " Document(page_content='of BERT’s attention focuses on the deliminator to-\\nken [SEP], which we argue is used by the model\\nas a sort of no-op. Generally we ﬁnd that attention\\nheads in the same layer tend to behave similarly.\\nWe next probe each attention head for linguistic\\nphenomena. In particular, we treat each head as a\\nsimple no-training-required classiﬁer that, given a\\nword as input, outputs the most-attended-to other\\nword. We then evaluate the ability of the heads\\nto classify various syntactic relations. While no\\nsingle head performs well at many relations, we\\nﬁnd that particular heads correspond remarkably', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 0}),\n",
       " Document(page_content='well to particular relations. For example, we ﬁnd\\nheads that ﬁnd direct objects of verbs, determin-\\ners of nouns, objects of prepositions, and objects\\nof possessive pronouns with >75% accuracy. We\\nperform a similar analysis for coreference resolu-\\ntion, also ﬁnding a BERT head that performs quite\\nwell. These results are intriguing because the be-\\nhavior of the attention heads emerges purely from\\nself-supervised training on unlabeled data, without\\nexplicit supervision for syntax or coreference.', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 0}),\n",
       " Document(page_content='havior of the attention heads emerges purely from\\nself-supervised training on unlabeled data, without\\nexplicit supervision for syntax or coreference.\\n1Code will be released at https://github.com/\\nclarkkev/attention-analysis .\\n2We use the English base-sized model.', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 0}),\n",
       " Document(page_content='Head 1-1  Attends broadly   Head 3-1  Attends to next token   Head 8-7  Attends to [SEP]   Head 11-6  Attends to periods   Figure 1: Examples of heads exhibiting the patterns discussed in Section 3. The darkness of a line indicates the\\nstrength of the attention weight (some attention weights are so low they are invisible).\\nOur ﬁndings show that particular heads special-\\nize to speciﬁc aspects of syntax. To get a more\\noverall measure of the attention heads’ syntac-\\ntic ability, we propose an attention-based probing\\nclassiﬁer that takes attention maps as input. The', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 1}),\n",
       " Document(page_content='classiﬁer achieves 77 UAS at dependency pars-\\ning, showing BERT’s attention captures a substan-\\ntial amount about syntax. Several recent works\\nhave proposed incorporating syntactic information\\nto improve attention (Eriguchi et al., 2016; Chen\\net al., 2018; Strubell et al., 2018). Our work sug-\\ngests that to an extent this kind of syntax-aware\\nattention already exists in BERT, which may be\\none of the reason for its success.\\n2 Background: Transformers and BERT\\nAlthough our analysis methods are applicable\\nto any model that uses an attention mechanism,', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 1}),\n",
       " Document(page_content='in this paper we analyze BERT (Devlin et al.,\\n2019), a large Transformer (Vaswani et al., 2017)\\nnetwork. Transformers consist of multiple lay-\\ners where each layer contains multiple attention\\nheads. An attention head takes as input a sequence\\nof vectorsh= [h1,...,h n]corresponding to the\\nntokens of the input sentence. Each vector hi\\nis transformed into query, key, and value vectors\\nqi,ki,vithrough separate linear transformations.\\nThe head computes attention weights αbetween\\nall pairs of words as softmax-normalized dot prod-\\nucts between the query and key vectors. The out-', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 1}),\n",
       " Document(page_content='putoof the attention head is a weighted sum of the\\nvalue vectors.\\nαij=exp (qT\\nikj)∑n\\nl=1exp (qT\\nikl)oi=n∑\\nj=1αijvjAttention weights can be viewed as governing how\\n“important” every other token is when producing\\nthe next representation for the current token.\\nBERT is pre-trained on 3.3 billion tokens of En-\\nglish text to perform two tasks. In the “masked\\nlanguage modeling” task, the model predicts the\\nidentities of words that have been masked-out of\\nthe input text. In the “next sentence prediction”\\ntask, the model predicts whether the second half', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 1}),\n",
       " Document(page_content='of the input follows the ﬁrst half of the input in the\\ncorpus, or is a random paragraph. Further training\\nthe model on supervised data results in impres-\\nsive performance across a variety of tasks rang-\\ning from sentiment analysis to question answering.\\nAn important detail of BERT is the preprocessing\\nused for the input text. A special token [CLS] is\\nadded to the beginning of the text and another to-\\nken [SEP] is added to the end. If the input consists\\nof multiple separate texts (e.g., a reading compre-\\nhension example consists of a separate question', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 1}),\n",
       " Document(page_content='and context), [SEP] tokens are also used to sep-\\narate them. As we show in the next section, these\\nspecial tokens play an important role in BERT’s\\nattention. We use the “base” sized BERT model,\\nwhich has 12 layers containing 12 attention heads\\neach. We use <layer>-<head number>to denote\\na particular attention head.\\n3 Surface-Level Patterns in Attention\\nBefore looking at speciﬁc linguistic phenomena,\\nwe ﬁrst perform an analysis of surface-level pat-\\nterns in how BERT’s attention heads behave. Ex-\\namples of heads exhibiting these patterns are\\nshown in Figure 1.', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 1}),\n",
       " Document(page_content='2 4 6 8 10 12\\nLayer0.00.20.40.60.8Avg. Attention\\n[CLS]\\n[SEP]\\n. or ,\\n2 4 6 8 10 12\\nLayer0.00.20.40.60.81.0Avg. Attention\\n[SEP] -> [SEP]\\nother -> [SEP]Figure 2: Each point corresponds to the average atten-\\ntion a particular BERT attention head puts toward a to-\\nken type. Above: heads often attend to “special” to-\\nkens. Early heads attend to [CLS], middle heads attend\\nto [SEP], and deep heads attend to periods and com-\\nmas. Often more than half of a head’s total attention is\\nto these tokens. Below: heads attend to [SEP] tokens\\neven more when the current token is [SEP] itself.', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 2}),\n",
       " Document(page_content='Setup. We extract the attention maps from BERT-\\nbase over 1000 random Wikipedia segments. We\\nfollow the setup used for pre-training BERT where\\neach segment consists of at most 128 tokens\\ncorresponding to two consecutive paragraphs of\\nWikipedia (although we do not mask out in-\\nput words or as in BERT’s training). The in-\\nput presented to the model is [CLS] <paragraph-\\n1>[SEP]<paragraph-2>[SEP].\\n3.1 Relative Position\\nFirst, we compute how often BERT’s attention\\nheads attend to the current token, the previous to-\\nken, or the next token. We ﬁnd that most heads', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 2}),\n",
       " Document(page_content='put little attention on the current token. However,\\nthere are heads that specialize to attending heavily\\non the next or previous token, especially in ear-\\nlier layers of the network. In particular four atten-\\ntion heads (in layers 2, 4, 7, and 8) on average put\\n>50% of their attention on the previous token and\\nﬁve attention heads (in layers 1, 2, 2, 3, and 6) put\\n>50% of their attention on the next token.\\n2 4 6 8 10 12\\nLayer0.00.51.01.52.02.53.0Average⏐⏐∂L\\n∂α⏐⏐All unmasked tokens\\n[SEP]\\n. or ,Figure 3: Gradient-based feature importance estimates', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 2}),\n",
       " Document(page_content='for attention to [SEP], periods/commas, and other to-\\nkens.\\n2 4 6 8 10 12\\nLayer024Avg. Attention Entropy (nats)\\nuniform attention\\nBERT Heads\\nFigure 4: Entropies of attention distributions. In the\\nﬁrst layer there are particularly high-entropy heads that\\nproduce bag-of-vector-like representations.\\n3.2 Attending to Separator Tokens\\nInterestingly, we found that a substantial amount\\nof BERT’s attention focuses on a few tokens (see\\nFigure 2). For example, over half of BERT’s atten-\\ntion in layers 6-10 focuses on [SEP]. To put this in\\ncontext, since most of our segments are 128 tokens', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 2}),\n",
       " Document(page_content='long, the average attention for a token occurring\\ntwice in a segments like [SEP] would normally be\\naround 1/64. [SEP] and [CLS] are guaranteed to\\nbe present and are never masked out, while pe-\\nriods and commas are the most common tokens\\nin the data excluding “the,” which might be why\\nthe model treats these tokens differently. A sim-\\nilar pattern occurs for the uncased BERT model,\\nsuggesting there is a systematic reason for the at-\\ntention to special tokens rather than it being an ar-\\ntifact of stochastic training.\\nOne possible explanation is that [SEP] is used', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 2}),\n",
       " Document(page_content='to aggregate segment-level information which can\\nthen be read by other heads. However, further\\nanalysis makes us doubtful this is the case. If this', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 2}),\n",
       " Document(page_content='explanation were true, we would expect attention\\nheads processing [SEP] to attend broadly over the\\nwhole segment to build up these representations.\\nHowever, they instead almost entirely (more than\\n90%; see bottom of Figure 2) attend to themselves\\nand the other [SEP] token. Furthermore, qualita-\\ntive analysis (see Figure 5) shows that heads with\\nspeciﬁc functions attend to [SEP] when the func-\\ntion is not called for. For example, in head 8-10\\ndirect objects attend to their verbs. For this head,\\nnon-nouns mostly attend to [SEP]. Therefore, we\\nspeculate that attention over these special tokens', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 3}),\n",
       " Document(page_content='might be used as a sort of “no-op” when the atten-\\ntion head’s function is not applicable.\\nTo further investigate this hypothesis, we ap-\\nply gradient-based measures of feature importance\\n(Sundararajan et al., 2017). In particular, we com-\\npute the magnitude of the gradient of the loss from\\nBERT’s masked language modeling task with re-\\nspect to each attention weight. Intuitively, this\\nvalue measures how much changing the attention\\nto a token will change BERT’s outputs. Results\\nare shown in Figure 3. Starting in layer 5 – the\\nsame layer where attention to [SEP] becomes high', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 3}),\n",
       " Document(page_content='– the gradients for attention to [SEP] become very\\nsmall. This indicates that attending more or less to\\n[SEP] does not substantially change BERT’s out-\\nputs, supporting the theory that attention to [SEP]\\nis used as a no-op for attention heads.\\n3.3 Focused vs Broad Attention\\nLastly, we measure whether attention heads fo-\\ncus on a few words or attend broadly over many\\nwords. To do this, we compute the average en-\\ntropy of each head’s attention distribution (see\\nFigure 4). We ﬁnd that some attention heads, es-\\npecially in lower layers, have very broad atten-', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 3}),\n",
       " Document(page_content='tion. These high-entropy attention heads typically\\nspend at most 10% of their attention mass on any\\nsingle word. The output of these heads is roughly\\na bag-of-vectors representation of the sentence.\\nWe also measured entropies for all attention\\nheads from only the [CLS] token. While the av-\\nerage entropies from [CLS] for most layers are\\nvery close to the ones shown in Figure 4, the\\nlast layer has a high entropy from [CLS] of 3.89\\nnats, indicating very broad attention. This ﬁnd-\\ning makes sense given that the representation for\\nthe [CLS] token is used as input for the “next sen-', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 3}),\n",
       " Document(page_content='tence prediction” task during pre-training, so it at-\\ntends broadly to aggregate a representation for thewhole input in the last layer.\\n4 Probing Individual Attention Heads\\nNext, we investigate individual attention heads to\\nprobe what aspects of language they have learned.\\nIn particular, we evaluate attention heads on la-\\nbeled datasets for tasks like dependency parsing.\\nAn overview of our results is shown in Figure 5.\\n4.1 Method\\nWe wish to evaluate attention heads at word-level\\ntasks, but BERT uses byte-pair tokenization (Sen-\\nnrich et al., 2016), which means some words', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 3}),\n",
       " Document(page_content='(∼8% in our data) are split up into multiple to-\\nkens. We therefore convert token-token attention\\nmaps to word-word attention maps. For attention\\ntoa split-up word, we sum up the attention weights\\nover its tokens. For attention from a split-up word,\\nwe take the mean of the attention weights over its\\ntokens. These transformations preserve the prop-\\nerty that the attention from each word sums to\\n1. For a given attention head and word, we take\\nwhichever other word receives the most attention\\nweight as that model’s prediction3\\n4.2 Dependency Syntax\\nSetup. We extract attention maps from BERT on', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 3}),\n",
       " Document(page_content='the Wall Street Journal portion of the Penn Tree-\\nbank (Marcus et al., 1993) annotated with Stanford\\nDependencies. We evaluate both “directions” of\\nprediction for each attention head: the head word\\nattending to the dependent and the dependent at-\\ntending to the head word. Some dependency rela-\\ntions are simpler to predict than others: for exam-\\nple a noun’s determiner is often the immediately\\npreceding word. Therefore as a point of compar-\\nison, we show predictions from a simple ﬁxed-', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 3}),\n",
       " Document(page_content='tions are simpler to predict than others: for exam-\\nple a noun’s determiner is often the immediately\\npreceding word. Therefore as a point of compar-\\nison, we show predictions from a simple ﬁxed-\\noffset baseline. For example, a ﬁxed offset of -2\\nmeans the word two positions to the left of the de-\\npendent is always considered to be the head.\\nResults. Table 1 shows that there is no single at-\\ntention head that does well at syntax “overall”; the\\nbest head gets 34.5 UAS, which is not much better\\nthan the right-branching baseline, which gets 26.3\\nUAS. This ﬁnding is similar to the one reported by', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 3}),\n",
       " Document(page_content='Raganato and Tiedemann (2018), who also evalu-\\nate individual attention heads for syntax.\\nHowever, we do ﬁnd that certain attention heads\\nspecialize to speciﬁc dependency relations, some-\\n3We ignore [SEP] and [CLS], although in practice this\\ndoes not signiﬁcantly change the accuracies for most heads.', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 3}),\n",
       " Document(page_content='Head 9-6  - Prepositions attend to their objects  - 76.3% accuracy at the pobj relation Head 8-11  - Noun modifiers (e.g., determiners) attend   to their noun  - 94.3% accuracy at the det relation Head 8-10  - Direct objects attend to their verbs  - 86.8% accuracy at the dobj relation \\nHead 7-6  - Possessive pronouns and apostrophes   attend to the head of the corresponding NP  - 80.5% accuracy at the poss relation Head 4-10  - Passive auxiliary verbs attend to the   verb they modify  - 82.5% accuracy at the auxpass relation', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 4}),\n",
       " Document(page_content='Head 5-4  - Coreferent mentions attend to their antecedents  - 65.1% accuracy at linking the head of a    coreferent mention to the head of an antecedent Figure 5: BERT attention heads that correspond to linguistic phenomena. In the example attention maps, the\\ndarkness of a line indicates the strength of the attention weight. All attention to/from red words is colored red;\\nthese colors are there to highlight certain parts of the attention heads’ behaviors. For Head 9-6, we don’t show', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 4}),\n",
       " Document(page_content='attention to [SEP] for clarity. Despite not being explicitly trained on these tasks, BERT’s attention heads perform\\nremarkably well, illustrating how syntax-sensitive behavior can emerge from self-supervised training alone.', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 4}),\n",
       " Document(page_content='Relation Head Accuracy Baseline\\nAll 7-6 34.5 26.3 (1)\\nprep 7-4 66.7 61.8 (-1)\\npobj 9-6 76.3 34.6 (-2)\\ndet 8-11 94.3 51.7 (1)\\nnn 4-10 70.4 70.2 (1)\\nnsubj 8-2 58.5 45.5 (1)\\namod 4-10 75.6 68.3 (1)\\ndobj 8-10 86.8 40.0 (-2)\\nadvmod 7-6 48.8 40.2 (1)\\naux 4-10 81.1 71.5 (1)\\nposs 7-6 80.5 47.7 (1)\\nauxpass 4-10 82.5 40.5 (1)\\nccomp 8-1 48.8 12.4 (-2)\\nmark 8-2 50.7 14.5 (2)\\nprt 6-7 99.1 91.4 (-1)\\nTable 1: The best performing attentions heads of\\nBERT on WSJ dependency parsing by dependency\\ntype. Numbers after baseline accuracies show the best\\noffset found (e.g., (1) means the word to the right is', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 5}),\n",
       " Document(page_content='predicted as the head). We show the 10 most common\\nrelations as well as 5 other ones attention heads do well\\non. Bold highlights particularly effective heads.\\ntimes achieving high accuracy and substantially\\noutperforming the ﬁxed-offset baseline. We ﬁnd\\nthat for all relations in Table 1 except pobj , the\\ndependent attends to the head word rather than the\\nother way around, likely because each dependent\\nhas exactly one head but heads have multiple de-\\npendents. We also note heads can disagree with\\nstandard annotation conventions while still per-\\nforming syntactic behavior. For example, head 7-', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 5}),\n",
       " Document(page_content='6 marks ’sas the dependent for the poss relation,\\nwhile gold-standard labels mark the complement\\nof an ’sas the dependent (the accuracy in Table 1\\ncounts ’sas correct). Such disagreements high-\\nlight how these syntactic behaviors in BERT are\\nlearned as a by-product of self-supervised train-\\ning, not by copying a human design.\\nFigure 5 shows some examples of the attention\\nbehavior. While the similarity between machine-\\nlearned attention weights and human-deﬁned syn-\\ntactic relations are striking, we note these are re-\\nlations for which attention heads do particularly', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 5}),\n",
       " Document(page_content='well on. There are many relations for which BERT\\nonly slightly improves over the simple baseline, so\\nwe would not say individual attention heads cap-\\nture dependency structure as a whole. We think\\nit would be interesting future work to extend ouranalysis to see if the relations well-captured by at-\\ntention are similar or different for other languages.\\n4.3 Coreference Resolution\\nHaving shown BERT attention heads reﬂect cer-\\ntain aspects of syntax, we now explore using at-\\ntention heads for the more challenging semantic\\ntask of coreference resolution. Coreference links', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 5}),\n",
       " Document(page_content='are usually longer than syntactic dependencies and\\nstate-of-the-art systems generally perform much\\nworse at coreference compared to parsing.\\nSetup. We evaluate the attention heads on coref-\\nerence resolution using the CoNLL-2012 dataset4\\n(Pradhan et al., 2012). In particular, we compute\\nantecedent selection accuracy: what percent of the\\ntime does the head word of a coreferent mention\\nmost attend to the head of one of that mention’s\\nantecedents. We compare against three baselines\\nfor selecting an antecedent:\\n•Picking the nearest other mention.\\n•Picking the nearest other mention with the', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 5}),\n",
       " Document(page_content='same head word as the current mention.\\n•A simple rule-based system inspired by Lee\\net al. (2011). It proceeds through 4 sieves: (1)\\nfull string match, (2) head word match, (3)\\nnumber/gender/person match, (4) all other\\nmentions. The nearest mention satisfying the\\nearliest sieve is returned.\\nWe also show the performance of a recent neural\\ncoreference system from Wiseman et al. (2015).\\nResults. Results are shown in Table 2. We ﬁnd\\nthat one of BERT’s attention heads achieves de-\\ncent coreference resolution performance, improv-\\ning by over 10 accuracy points on the string-', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 5}),\n",
       " Document(page_content='matching baseline and performing close to the\\nrule-based system. It is particularly good with\\nnominal mentions, perhaps because it is capable\\nof fuzzy matching between synonyms as seen in\\nthe bottom right of Figure 5.\\n5 Probing Attention Head Combinations\\nSince individual attention heads specialize to par-\\nticular aspects of syntax, the model’s overall\\n“knowledge” about syntax is distributed across\\nmultiple attention heads. We now measure this\\noverall ability by proposing a novel family of', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 5}),\n",
       " Document(page_content='ticular aspects of syntax, the model’s overall\\n“knowledge” about syntax is distributed across\\nmultiple attention heads. We now measure this\\noverall ability by proposing a novel family of\\nattention-based probing classiﬁers and applying\\n4We truncate documents to 128 tokens long to keep mem-\\nory usage manageable.', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 5}),\n",
       " Document(page_content='Model All Pronoun Proper Nominal\\nNearest 27 29 29 19\\nHead match 52 47 67 40\\nRule-based 69 70 77 60\\nNeural coref 83* – – –\\nHead 5-4 65 64 73 58\\n*Only roughly comparable because on non-truncated docu-\\nments and with different mention detection.\\nTable 2: Accuracies (%) for systems at selecting a\\ncorrect antecedent given a coreferent mention in the\\nCoNLL-2012 data. One of BERT’s attention heads per-\\nforms fairly well at coreference.\\nthem to dependency parsing. For these classiﬁers\\nwe treat the BERT attention outputs as ﬁxed, i.e.,\\nwe do not back-propagate into BERT and only', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 6}),\n",
       " Document(page_content='train a small number of parameters.\\nThe probing classiﬁers are basically graph-\\nbased dependency parsers. Given an input word,\\nthe classiﬁer produces a probability distribution\\nover other words in the sentence indicating how\\nlikely each other word is to be the syntactic head\\nof the current one.\\nAttention-Only Probe. Our ﬁrst probe learns a\\nsimple linear combination of attention weights.\\np(i|j)∝exp(n∑\\nk=1wkαk\\nij+ukαk\\nji)\\nwherep(i|j)is the probability of word ibeing\\nwordj’s syntactic head, αk\\nijis the attention weight\\nfrom wordito wordjproduced by head k, andn', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 6}),\n",
       " Document(page_content='is the number of attention heads. We include both\\ndirections of attention: candidate head to depen-\\ndent as well as dependent to candidate head. The\\nweight vectors wanduare trained using standard\\nsupervised learning on the train set.\\nAttention-and-Words Probe. Given our ﬁnding\\nthat heads specialize to particular syntactic rela-\\ntions, we believe probing classiﬁers should beneﬁt\\nfrom having information about the input words. In\\nparticular, we build a model that sets the weights\\nof the attention heads based on the GloVe (Pen-\\nnington et al., 2014) embeddings for the input', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 6}),\n",
       " Document(page_content='words. Intuitively, if the dependent and candi-\\ndate head are “the” and “cat,” the probing classi-\\nﬁer should learn to assign most of the weight to\\nthe head 8-11, which achieves excellent perfor-\\nmance at the determiner relation. The attention-and-words probing classiﬁer assigns the probabil-\\nity of wordibeing wordj’s head as\\np(i|j)∝exp(n∑\\nk=1Wk,:(vi⊕vj)αk\\nij+\\nUk,:(vi⊕vj)αk\\nji)\\nWherevdenotes GloVe embeddings and ⊕de-\\nnotes concatenation. The GloVe embeddings are\\nheld ﬁxed in training, so only the two weight ma-\\ntricesWandUare learned. The dot product', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 6}),\n",
       " Document(page_content='Wk,:(vi⊕vj)produces a word-sensitive weight for\\nthe particular attention head.\\nResults. We evaluate our methods on the Penn\\nTreebank dev set annotated with Stanford depen-\\ndencies. We compare against three baselines:\\n•A right-branching baseline that always pre-\\ndicts the head is to the dependent’s right.\\n•A simple one-hidden-layer network that takes\\nas input the GloVe embeddings for the depen-\\ndent and candidate head as well as distance\\nfeatures between the two words.5\\n•Our attention-and-words probe, but with at-\\ntention maps from a BERT network with pre-', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 6}),\n",
       " Document(page_content='trained word/positional embeddings but ran-\\ndomly initialized other weights. This kind of\\nbaseline is surprisingly strong at other prob-\\ning tasks (Conneau et al., 2018).\\nResults are shown in Table 3. We ﬁnd the Attn +\\nGloVe probing classiﬁer substantially outperforms\\nour baselines and achieves a decent UAS of 77,\\nsuggesting BERT’s attention maps have a fairly\\nthorough representation of English syntax.\\nAs a rough comparison, we also report results\\nfrom the structural probe from Hewitt and Man-\\nning (2019), which builds a probing classiﬁer on\\ntop of BERT’s vector representations rather than', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 6}),\n",
       " Document(page_content='attention. The scores are not directly compara-\\nble because the structural probe only uses a sin-\\ngle layer of BERT, produces undirected rather than\\ndirected parse trees, and is trained to produce the\\nsyntactic distance between words rather than di-\\nrectly predicting the tree structure. Nevertheless,\\nthe similarity in score to our Attn + Glove probing\\nclassiﬁer suggests there is not much more syntac-\\ntic information in BERT’s vector representations\\ncompared to its attention maps.\\n5Indicator features for short distances as well as continu-', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 6}),\n",
       " Document(page_content='classiﬁer suggests there is not much more syntac-\\ntic information in BERT’s vector representations\\ncompared to its attention maps.\\n5Indicator features for short distances as well as continu-\\nous distance features, with distance ahead/behind treated sep-\\narately to capture word order', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 6}),\n",
       " Document(page_content='Model UAS\\nStructural probe 80 UUAS*\\nRight-branching 26\\nDistances + GloVe 58\\nRandom Init Attn + GloVe 30\\nAttn 61\\nAttn + GloVe 77\\nTable 3: Results of attention-based probing classiﬁers\\non dependency parsing. A simple model taking BERT\\nattention maps and GloVe embeddings as input per-\\nforms quite well. *Not directly comparable to our num-\\nbers; see text.\\nOverall, our results from probing both individ-\\nual and combinations of attention heads suggest\\nthat BERT learns some aspects syntax purely as a\\nby-product of self-supervised training. Other work\\nhas drawn a similar conclusions from examin-', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 7}),\n",
       " Document(page_content='ing BERT’s predictions on agreement tasks (Gold-\\nberg, 2019) or internal vector representations (He-\\nwitt and Manning, 2019; Liu et al., 2019). Tra-\\nditionally, syntax-aware models have been devel-\\noped through architecture design (e.g., recursive\\nneural networks) or from direct supervision from\\nhuman-curated treebanks. Our ﬁndings are part of\\na growing body of work indicating that indirect\\nsupervision from rich pre-training tasks like lan-\\nguage modeling can also produce models sensitive\\nto language’s hierarchical structure.\\n6 Clustering Attention Heads', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 7}),\n",
       " Document(page_content='6 Clustering Attention Heads\\nAre attention heads in the same layer similar to\\neach other or different? Can attention heads be\\nclearly grouped by behavior? We investigate these\\nquestions by computing the distances between all\\npairs of attention heads. Formally, we measure the\\ndistance between two heads H iand H jas:\\n∑\\ntoken∈dataJS(Hi(token ),Hj(token ))\\nWhereJSis the Jensen-Shannon Divergence be-\\ntween attention distributions. Using these dis-\\ntances, we visualize the attention heads by apply-\\ning multidimensional scaling (Kruskal, 1964) to\\nembed each head in two dimensions such that the', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 7}),\n",
       " Document(page_content='Euclidean distance between embeddings reﬂects\\nthe Jensen-Shannon distance between the corre-\\nsponding heads as closely as possible.\\nResults are shown in Figure 6. We ﬁnd that\\nthere are several clear clusters of heads that be-\\nFigure 6: BERT attention heads embedded in two-\\ndimensional space. Distances between points approxi-\\nmately match the average Jensen-Shannon divergences\\nbetween the outputs of the corresponding heads. Heads\\nin the same layer tend to be close together. Attention\\nhead “behavior” was found through the analysis meth-\\nods discussed throughout this paper.', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 7}),\n",
       " Document(page_content='have similarly, often corresponding to behaviors\\nwe have already discussed in this paper. Heads\\nwithin the same layer are often fairly close to each\\nother, meaning that heads within the layer have\\nsimilar attention distributions. This ﬁnding is a bit\\nsurprising given that Tu et al. (2018) show that en-\\ncouraging attention heads to have different behav-\\niors can improve Transformer performance at ma-\\nchine translation. One possibility for the apparent\\nredundancy in BERT’s attention heads is the use', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 7}),\n",
       " Document(page_content='of attention dropout, which causes some attention\\nweights to be zeroed-out during training.\\n7 Related Work\\nThere has been substantial recent work perform-\\ning analysis to better understand what neural net-\\nworks learn, especially from language model pre-\\ntraining. One line of research examines the out-\\nputs of language models on carefully chosen in-\\nput sentences (Linzen et al., 2016; Khandelwal\\net al., 2018; Gulordava et al., 2018; Marvin and\\nLinzen, 2018). For example, the model’s perfor-\\nmance at subject-verb agreement (generating the\\ncorrect number of a verb far away from its sub-', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 8}),\n",
       " Document(page_content='ject) provides a measure of the model’s syntactic\\nability, although it does not reveal how that ability\\nis captured by the network.\\nAnother line of work investigates the internal\\nvector representations of the model (Adi et al.,\\n2017; Giulianelli et al., 2018; Zhang and Bow-\\nman, 2018), often using probing classiﬁers. Prob-\\ning classiﬁers are simple neural networks that take\\nthe vector representations of a pre-trained model\\nas input and are trained to do a supervised task\\n(e.g., part-of-speech tagging). If a probing clas-\\nsiﬁer achieves high accuracy, it suggests that the', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 8}),\n",
       " Document(page_content='input representations reﬂect the corresponding as-\\npect of language (e.g., low-level syntax). Like\\nour work, some of these studies have also demon-\\nstrated models capturing aspects of syntax (Shi\\net al., 2016; Blevins et al., 2018) or coreference\\n(Tenney et al., 2018, 2019; Liu et al., 2019) with-\\nout explicitly being trained for the tasks.\\nWith regards to analyzing attention, Vig (2019)\\nbuilds a visualization tool for the BERT’s atten-\\ntion and reports observations about the attention\\nbehavior, but does not perform quantitative anal-\\nysis. Burns et al. (2018) analyze the attention', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 8}),\n",
       " Document(page_content='of memory networks to understand model perfor-\\nmance on a question answering dataset. There has\\nalso been some initial work in correlating atten-\\ntion with syntax. Raganato and Tiedemann (2018)\\nevaluate the attention heads of a machine trans-\\nlation model on dependency parsing, but only re-\\nport overall UAS scores instead of investigating\\nheads for speciﬁc syntactic relations or using prob-\\ning classiﬁers. Marecek and Rosa (2018) propose\\nheuristic ways of converting attention scores to\\nsyntactic trees, but do not quantitatively evaluate\\ntheir approach. For coreference, V oita et al. (2018)', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 8}),\n",
       " Document(page_content='show that the attention of a context-aware neu-ral machine translation system captures anaphora,\\nsimilar to our ﬁnding for BERT.\\nConcurrently with our work V oita et al. (2019)\\nidentify syntactic, positional, and rare-word-\\nsensitive attention heads in machine translation\\nmodels. They also demonstrate that many atten-\\ntion heads can be pruned away without substan-\\ntially hurting model performance. Interestingly,\\nthe important attention heads that remain after\\npruning tend to be ones with identiﬁed behaviors.\\nMichel et al. (2019) similarly show that many of', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 8}),\n",
       " Document(page_content='BERT’s attention heads can be pruned. Although\\nour analysis in this paper only found interpretable\\nbehaviors in a subset of BERT’s attention heads,\\nthese recent works suggest that there might not be\\nmuch to explain for some attention heads because\\nthey have little effect on model perfomance.\\nJain and Wallace (2019) argue that attention of-\\nten does not “explain” model predictions. They\\nshow that attention weights frequently do not cor-\\nrelate with other measures of feature importance.\\nFurthermore, attention weights can often be sub-\\nstantially changed without altering model predic-', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 8}),\n",
       " Document(page_content='tions. However, our motivation for looking at at-\\ntention is different: rather than explaining model\\npredictions, we are seeking to understand infor-\\nmation learned by the models. For example, if\\na particular attention head learns a syntactic rela-\\ntion, we consider that an important ﬁnding from\\nan analysis perspective even if that head is not\\nalways used when making predictions for some\\ndownstream task.\\n8 Conclusion\\nWe have proposed a series of analysis methods for', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 8}),\n",
       " Document(page_content='an analysis perspective even if that head is not\\nalways used when making predictions for some\\ndownstream task.\\n8 Conclusion\\nWe have proposed a series of analysis methods for\\nunderstanding the attention mechanisms of mod-\\nels and applied them to BERT. While most recent\\nwork on model analysis for NLP has focused on\\nprobing vector representations or model outputs,\\nwe have shown that a substantial amount of lin-\\nguistic knowledge can be found not only in the\\nhidden states, but also in the attention maps. We\\nthink probing attention maps complements these', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 8}),\n",
       " Document(page_content='other model analysis techniques, and should be\\npart of the toolkit used by researchers to under-\\nstand what neural networks learn about language.\\nAcknowledgements\\nWe thank the anonymous reviews for their\\nthoughtful comments and suggestions. Kevin is\\nsupported by a Google PhD Fellowship.', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 8}),\n",
       " Document(page_content='References\\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer\\nLavi, and Yoav Goldberg. 2017. Fine-grained anal-\\nysis of sentence embeddings using auxiliary predic-\\ntion tasks. In ICLR .\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\\ngio. 2015. Neural machine translation by jointly\\nlearning to align and translate. In ICLR .\\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Has-\\nsan Sajjad, and James R. Glass. 2017. What do neu-\\nral machine translation models learn about morphol-\\nogy? In ACL.\\nTerra Blevins, Omer Levy, and Luke S. Zettlemoyer.\\n2018. Deep rnns encode soft hierarchical syntax. In\\nACL.', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 9}),\n",
       " Document(page_content='ACL.\\nKaylee Burns, Aida Nematzadeh, Alison Gopnik, and\\nThomas L. Grifﬁths. 2018. Exploiting attention to\\nreveal shortcomings in memory models. In Black-\\nboxNLP@EMNLP .\\nKehai Chen, Rui Wang, Masao Utiyama, Eiichiro\\nSumita, and Tiejun Zhao. 2018. Syntax-directed at-\\ntention for neural machine translation. In AAAI .\\nAlexis Conneau, Germ ´an Kruszewski, Guillaume\\nLample, Lo ¨ıc Barrault, and Marco Baroni. 2018.\\nWhat you can cram into a single $&!#* vector:\\nProbing sentence embeddings for linguistic proper-\\nties. In ACL.\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 9}),\n",
       " Document(page_content='sequence learning. In NIPS .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In NAACL-HLT .\\nAkiko Eriguchi, Kazuma Hashimoto, and Yoshimasa\\nTsuruoka. 2016. Tree-to-sequence attentional neu-\\nral machine translation. In ACL.\\nMario Giulianelli, Jack Harding, Florian Mohnert,\\nDieuwke Hupkes, and Willem H. Zuidema. 2018.\\nUnder the hood: Using diagnostic classiﬁers to in-\\nvestigate and improve how language models track\\nagreement information. In BlackboxNLP@EMNLP .', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 9}),\n",
       " Document(page_content='Yoav Goldberg. 2019. Assessing BERT’s syntactic\\nabilities. arXiv preprint arXiv:1901.05287 .\\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\\nTal Linzen, and Marco Baroni. 2018. Colorless\\ngreen recurrent networks dream hierarchically. In\\nNAACL-HLT .\\nJohn Hewitt and Christopher D. Manning. 2019. Find-\\ning syntax with structural probes. In NAACL-HLT .\\nSarthak Jain and Byron C. Wallace. 2019. Attention is\\nnot explanation. arXiv preprint arXiv:1902.10186 .Urvashi Khandelwal, He He, Peng Qi, and Daniel Ju-\\nrafsky. 2018. Sharp nearby, fuzzy far away: How', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 9}),\n",
       " Document(page_content='neural language models use context. In ACL.\\nJoseph B Kruskal. 1964. Multidimensional scaling by\\noptimizing goodness of ﬁt to a nonmetric hypothe-\\nsis.Psychometrika , 29(1):1–27.\\nHeeyoung Lee, Yves Peirsman, Angel Chang,\\nNathanael Chambers, Mihai Surdeanu, and Dan Ju-\\nrafsky. 2011. Stanford’s multi-pass sieve corefer-\\nence resolution system at the conll-2011 shared task.\\nInCoNLL .\\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\\n2016. Assessing the ability of lstms to learn syntax-\\nsensitive dependencies. TACL .\\nNelson F. Liu, Matt Gardner, Yonatan Belinkov, M. Pe-', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 9}),\n",
       " Document(page_content='ters, and Noah A. Smith. 2019. Linguistic knowl-\\nedge and transferability of contextual representa-\\ntions. In NAACL-HLT .\\nMitchell P Marcus, Mary Ann Marcinkiewicz, and\\nBeatrice Santorini. 1993. Building a large annotated\\ncorpus of english: The Penn treebank. Computa-\\ntional linguistics , 19(2):313–330.\\nDavid Marecek and Rudolf Rosa. 2018. Extract-\\ning syntactic trees from transformer encoder self-\\nattentions. In BlackboxNLP@EMNLP .\\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\\ntactic evaluation of language models. In EMNLP .\\nPaul Michel, Omer Levy, and Graham Neubig. 2019.', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 9}),\n",
       " Document(page_content='Are sixteen heads really better than one? arXiv\\npreprint arXiv:1905.10650 .\\nJeffrey Pennington, Richard Socher, and Christopher\\nManning. 2014. Glove: Global vectors for word\\nrepresentation. In EMNLP .\\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\\nGardner, Christopher Clark, Kenton Lee, and Luke\\nZettlemoyer. 2018. Deep contextualized word rep-\\nresentations. In NAACL-HLT .\\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\\nOlga Uryupina, and Yuchen Zhang. 2012. Conll-\\n2012 shared task: Modeling multilingual unre-', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 9}),\n",
       " Document(page_content='resentations. In NAACL-HLT .\\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\\nOlga Uryupina, and Yuchen Zhang. 2012. Conll-\\n2012 shared task: Modeling multilingual unre-\\nstricted coreference in ontonotes. In Joint Confer-\\nence on EMNLP and CoNLL-Shared Task .\\nAlec Radford, Karthik Narasimhan, Tim Salimans,\\nand Ilya Sutskever. 2018. Improving lan-\\nguage understanding by generative pre-training.\\nhttps://blog.openai.com/language-unsupervised .\\nAlessandro Raganato and J ¨org Tiedemann. 2018.\\nAn analysis of encoder representations in\\ntransformer-based machine translation. In Black-\\nboxNLP@EMNLP .', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 9}),\n",
       " Document(page_content='boxNLP@EMNLP .\\nRico Sennrich, Barry Haddow, and Alexandra Birch.\\n2016. Neural machine translation of rare words with\\nsubword units. In ACL.', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 9}),\n",
       " Document(page_content='Xing Shi, Inkit Padhi, and Kevin Knight. 2016. Does\\nstring-based neural mt learn source syntax? In\\nEMNLP .\\nEmma Strubell, Patrick Verga, Daniel Andor,\\nDavid I Weiss, and Andrew McCallum. 2018.\\nLinguistically-informed self-attention for semantic\\nrole labeling. In EMNLP .\\nMukund Sundararajan, Ankur Taly, and Qiqi Yan.\\n2017. Axiomatic attribution for deep networks. In\\nICML .\\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\\nBert rediscovers the classical nlp pipeline. arXiv\\npreprint arXiv:1905.05950 .\\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\\nAdam Poliak, R Thomas McCoy, Najoung Kim,', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 10}),\n",
       " Document(page_content='Benjamin Van Durme, Samuel R Bowman, Dipan-\\njan Das, et al. 2018. What do you learn from con-\\ntext? probing for sentence structure in contextual-\\nized word representations. In ICLR .\\nZhaopeng Tu, Baosong Yang, Michael R. Lyu, and\\nTong Zhang. 2018. Multi-head attention with dis-\\nagreement regularization. In EMNLP .\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In NIPS .\\nJesse Vig. 2019. Visualizing attention in transformer-\\nbased language models. arXiv preprint\\narXiv:1904.02679 .', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 10}),\n",
       " Document(page_content='arXiv:1904.02679 .\\nElena V oita, Pavel Serdyukov, Rico Sennrich, and Ivan\\nTitov. 2018. Context-aware neural machine transla-\\ntion learns anaphora resolution. In ACL.\\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\\nnrich, and Ivan Titov. 2019. Analyzing multi-\\nhead self-attention: Specialized heads do the heavy\\nlifting, the rest can be pruned. arXiv preprint\\narXiv:1905.09418 .\\nSam Joshua Wiseman, Alexander Matthew Rush, Stu-\\nart Merrill Shieber, and Jason Weston. 2015. Learn-\\ning anaphoricity and antecedent ranking features for\\ncoreference resolution. In ACL.', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 10}),\n",
       " Document(page_content='Kelly W. Zhang and Samuel R. Bowman. 2018. Lan-\\nguage modeling teaches you more syntax than trans-\\nlation does: Lessons learned through auxiliary task\\nanalysis. In BlackboxNLP@EMNLP .', metadata={'source': '/Users/sidhaarthmurali/Desktop/Exela-Internship/Task-1-ChatBOT-for-PDFs/BERT_analysis.pdf', 'page': 10})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HUGGINGFACE EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USING PINECONE AS A VECTORSTORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pinecone-client in /Users/sidhaarthmurali/.local/share/virtualenvs/Task-1-ChatBOT-for-PDFs-w0uFnvha/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/sidhaarthmurali/.local/share/virtualenvs/Task-1-ChatBOT-for-PDFs-w0uFnvha/lib/python3.11/site-packages (from pinecone-client) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.4 in /Users/sidhaarthmurali/.local/share/virtualenvs/Task-1-ChatBOT-for-PDFs-w0uFnvha/lib/python3.11/site-packages (from pinecone-client) (6.0.1)\n",
      "Requirement already satisfied: loguru>=0.5.0 in /Users/sidhaarthmurali/.local/share/virtualenvs/Task-1-ChatBOT-for-PDFs-w0uFnvha/lib/python3.11/site-packages (from pinecone-client) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /Users/sidhaarthmurali/.local/share/virtualenvs/Task-1-ChatBOT-for-PDFs-w0uFnvha/lib/python3.11/site-packages (from pinecone-client) (4.7.1)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Users/sidhaarthmurali/.local/share/virtualenvs/Task-1-ChatBOT-for-PDFs-w0uFnvha/lib/python3.11/site-packages (from pinecone-client) (2.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/sidhaarthmurali/.local/share/virtualenvs/Task-1-ChatBOT-for-PDFs-w0uFnvha/lib/python3.11/site-packages (from pinecone-client) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /Users/sidhaarthmurali/.local/share/virtualenvs/Task-1-ChatBOT-for-PDFs-w0uFnvha/lib/python3.11/site-packages (from pinecone-client) (1.26.16)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /Users/sidhaarthmurali/.local/share/virtualenvs/Task-1-ChatBOT-for-PDFs-w0uFnvha/lib/python3.11/site-packages (from pinecone-client) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Users/sidhaarthmurali/.local/share/virtualenvs/Task-1-ChatBOT-for-PDFs-w0uFnvha/lib/python3.11/site-packages (from pinecone-client) (1.25.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/sidhaarthmurali/.local/share/virtualenvs/Task-1-ChatBOT-for-PDFs-w0uFnvha/lib/python3.11/site-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sidhaarthmurali/.local/share/virtualenvs/Task-1-ChatBOT-for-PDFs-w0uFnvha/lib/python3.11/site-packages (from requests>=2.19.0->pinecone-client) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sidhaarthmurali/.local/share/virtualenvs/Task-1-ChatBOT-for-PDFs-w0uFnvha/lib/python3.11/site-packages (from requests>=2.19.0->pinecone-client) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sidhaarthmurali/.local/share/virtualenvs/Task-1-ChatBOT-for-PDFs-w0uFnvha/lib/python3.11/site-packages (from requests>=2.19.0->pinecone-client) (2023.7.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pinecone      \n",
    "\n",
    "# pinecone.init(      \n",
    "# \tapi_key='ec5f7903-5240-4214-8104-1ab6cbf304fe',      \n",
    "# \tenvironment='us-west1-gcp-free'      \n",
    "# )      \n",
    "# index = ('llama2pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(documents= doc, embedding= embeddings)\n",
    "vectorstore.save_local(\"BERT-Learnings\")\n",
    "new_vectorstore = FAISS.load_local(\"BERT-Learnings\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
    "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: llama-cpp-python in /Users/sidhaarthmurali/.local/share/virtualenvs/Task-1-ChatBOT-for-PDFs-w0uFnvha/lib/python3.11/site-packages (0.1.77)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/sidhaarthmurali/.local/share/virtualenvs/Task-1-ChatBOT-for-PDFs-w0uFnvha/lib/python3.11/site-packages (from llama-cpp-python) (4.7.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/sidhaarthmurali/.local/share/virtualenvs/Task-1-ChatBOT-for-PDFs-w0uFnvha/lib/python3.11/site-packages (from llama-cpp-python) (1.25.2)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /Users/sidhaarthmurali/.local/share/virtualenvs/Task-1-ChatBOT-for-PDFs-w0uFnvha/lib/python3.11/site-packages (from llama-cpp-python) (5.6.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/sidhaarthmurali/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/47d28ef5de4f3de523c421f325a2e4e039035bab/llama-2-13b-chat.ggmlv3.q5_1.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 9 (mostly Q5_1)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 9677.07 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Loading model,\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    max_tokens=512,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains.question_answering import load_qa_chain\n",
    "qa = RetrievalQA.from_chain_type(llm = llm, chain_type = \"map_reduce\", retriever = new_vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as app:\n",
    "    chatbot = gr.Chatbot(label = \"BERTchat\")\n",
    "    msg = gr.Textbox(placeholder=\"Ask me anything about BERT?\")\n",
    "    clear = gr.ClearButton([msg, chatbot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as app:\n",
    "    chatbot = gr.Chatbot(label = \"BERTchat\")\n",
    "    msg = gr.Textbox(placeholder=\"Ask me anything about BERT?\")\n",
    "    clear = gr.ClearButton([msg, chatbot])\n",
    "\n",
    "    def respond(user_query, chat_history):\n",
    "        bot_message = qa.run(user_query)\n",
    "        chat_history.append((user_query, bot_message))\n",
    "        return \"\", chat_history\n",
    "        \n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Task-1-ChatBOT-for-PDFs-w0uFnvha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
